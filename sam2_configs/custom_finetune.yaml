# @package _global_

scratch:
  resolution: 640  # Resolution to resize images to
  train_batch_size: 1  # Start with a small batch size for stability
  num_train_workers: 4  # Adjust based on your CPU cores
  num_frames: 4  # Number of frames to process at once
  max_num_objects: 1  # Our dataset has single objects per video
  base_lr: 5.0e-6  # Start with a low learning rate
  vision_lr: 3.0e-6  # Lower learning rate for vision encoder
  phases_per_epoch: 1
  num_epochs: 10  # Start with fewer epochs for testing

dataset:
  # PATHS to Dataset - these will be set in the training script
  img_folder: null  # PATH to JPEGImages folder
  gt_folder: null   # PATH to Annotations folder
  file_list_txt: null  # PATH to file list containing videos to be used for training
  multiplier: 1

# Video transforms
vos:
  train_transforms:
    - _target_: training.dataset.transforms.ComposeAPI
      transforms:
        - _target_: training.dataset.transforms.RandomHorizontalFlip
          consistent_transform: True
        - _target_: training.dataset.transforms.RandomAffine
          degrees: 20
          shear: 10
          image_interpolation: bilinear
          consistent_transform: True
        - _target_: training.dataset.transforms.RandomResizeAPI
          sizes: ${scratch.resolution}
          square: true
          consistent_transform: True
        - _target_: training.dataset.transforms.ColorJitter
          consistent_transform: True
          brightness: 0.1
          contrast: 0.03
          saturation: 0.03
          hue: null
        - _target_: training.dataset.transforms.RandomGrayscale
          p: 0.05
          consistent_transform: True
        - _target_: training.dataset.transforms.ColorJitter
          consistent_transform: False
          brightness: 0.1
          contrast: 0.05
          saturation: 0.05
          hue: null
        - _target_: training.dataset.transforms.ToTensorAPI
        - _target_: training.dataset.transforms.NormalizeAPI
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]

trainer:
  _target_: training.trainer.Trainer
  mode: train_only
  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}}
  accelerator: cuda
  seed_value: 123

  model:
    _target_: training.model.sam2.SAM2Train
    # We'll use the base model for fine-tuning
    image_encoder_name: "facebook/sam2-hiera-base-plus"
    # Use higher resolution features in the SAM mask decoder
    use_high_res_features_in_sam: true
    # Output 3 masks on the first click on initial conditioning frames
    multimask_output_in_sam: true
    # SAM heads
    iou_prediction_use_sigmoid: True
    # Set image size based on our dataset
    image_size: ${scratch.resolution}

  # Optimizer configuration
  optim:
    _target_: training.optimizer.AdamWMultiGroup
    lr: ${scratch.base_lr}
    weight_decay: 0.01
    param_group_modifiers:
      - _target_: training.optimizer.ImageEncoderParamGroupModifier
        weight_decay: 0.001
        lr_mult: ${div:${scratch.vision_lr},${scratch.base_lr}}

  # Learning rate scheduler
  lr_scheduler:
    _target_: training.optimizer.CosineAnnealingLR
    T_max: ${minus:${trainer.max_epochs},1}
    eta_min: 1e-8

# Dataloaders configuration
data:
  train:
    _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset
    phases_per_epoch: ${phases_per_epoch}
    batch_sizes:
    - ${scratch.train_batch_size}
    datasets:
    # For Video Dataset (DAVIS-style)
    - _target_: training.dataset.vos_dataset.VOSDataset
      training: true
      video_dataset:
        _target_: training.dataset.vos_raw_dataset.DAVISRawDataset
        img_folder: ${dataset.img_folder}
        gt_folder: ${dataset.gt_folder}
        file_list_txt: ${dataset.file_list_txt}
        ann_every: 1  # Use every frame with annotation
      sampler:
        _target_: training.dataset.vos_sampler.RandomUniformSampler
        num_frames: ${scratch.num_frames}  # Number of frames per video sample
        max_num_objects: ${scratch.max_num_objects}
        reverse_time_prob: 0.5  # probability to reverse video
      transforms:
        _target_: training.dataset.transforms.VideoTransforms
        transform_params:
          min_resize_res: 640
          max_resize_res: 640  # Fixed resolution for our small dataset
          jitter_scale: [0.8, 1.2]
    shuffle: True
    num_workers: ${scratch.num_train_workers}
    pin_memory: True
    drop_last: True
    collate_fn:
      _target_: training.utils.data_utils.collate_fn
      _partial_: true
      dict_key: all

# Experiment logging
experiment_log_dir: "./sam2_logs/custom_finetune" 